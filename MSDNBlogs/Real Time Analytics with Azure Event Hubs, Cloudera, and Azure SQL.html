<html><head>
<meta charset='UTF-8'>
<link href='resource/bootstrap.min.css' rel='Stylesheet' type='text/css' />
<link href='resource/style.css' rel='Stylesheet' type='text/css' />
</head>
<body>
<div id='page'>
<h1 class='entry-title'>Real Time Analytics with Azure Event Hubs, Cloudera, and Azure SQL</h1>
 <a class='url fn n profile-usercard-hover' href='https://social.msdn.microsoft.com/profile/Paige Liu' target='_blank'>Paige Liu</a>
<time>    11/18/2015 5:42:00 PM</time>
<hr>
<div id='content'><p>In this blog post, I will demonstrate how to ingest data from Azure Event Hubs to Spark&nbsp;Streaming&nbsp;running on Cloudera EDH, process the data in real time using Spark SQL, and write the results to Azure SQL database.&nbsp; Alternatively, data&nbsp;processing can also be done using Impala.&nbsp; This example uses the same data generator as described in<a href="https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-get-started/" target="_blank"> this article about Azure Stream Analytics</a>.<h1><span style="font-size: small"><span style="font-size: large">Scenario:</span>&nbsp;</span></h1><p>A telecommunications company has a large volume of data for incoming calls.&nbsp; The company needs to -</p><ol>
<li>Obtain insights about customer usage aggregated across geographical regions in real time</li>
<li>Detect SIM fraud (multiple calls coming from the same identity around the same time but in geographically different locations) in real time</li>
</ol><h1><span style="font-size: large">Ingesting data:</span></h1><p>Data ingestion is the same&nbsp;as described in the aforementioned article about Stream Analytics.&nbsp;&nbsp;Follow these&nbsp;main steps:</p><h4><span style="font-size: small">Step 1: Create an Event Hubs Input</span>&nbsp;</h4><ol>
<li>Go to the <a href="http://manage.windowsazure.com" target="_blank">Azure portal</a>, click <strong>New </strong>-&gt; <strong>App Services </strong>-&gt; <strong>Service Bus </strong>-&gt; <strong>Event Hub </strong>-&gt; <strong>Quick Create</strong> to create a new Event Hub</li>
<li>Go to the newly created Event Hub, on the <strong>Consumer Groups</strong> tab, click <strong>Create</strong> at the bottom of the page to create a new consumer group</li>
<li>On the <strong>Configure</strong> tab of the Event Hub, under <strong>Shared Access Policies</strong>, create a new policy with <strong>Manage</strong> permissions</li>
<li>Save the changes, and navigate to the <strong>Dashboard</strong>, click <strong>Connection Information</strong> at the bottom of the page, and copy down the connection information</li>
</ol><h4><span style="font-size: small">Step 2: Download and configure the sample app&nbsp;that can generate sample&nbsp;call records and push them to Event Hubs</span></h4><ol>
<li>Download the event generator sample app from <a href="https://github.com/Azure/azure-stream-analytics/tree/master/DataGenerators/TelcoGenerator" target="_blank">GitHub</a>&nbsp;</li>
<li>Replace the Microsoft.ServiceBus.ConnectionString and EvenHubName values in App.Config with your Event Hub connection string and name</li>
<li>Run the solution</li>
</ol><p style="padding-left: 60px"><span style="color: #993366"><em>telcodatagen [#NumCDRsPerHour] [SIM Card Fraud Probability] [#DurationHours]</em></span></p><p style="padding-left: 30px">The following example will generate 1000 events with a 20% probability of fraud over the course of 2 hours</p><p style="padding-left: 60px"><span style="color: #993366"><em>telcodatagen 1000 .2 2</em></span></p><h4><span style="font-size: small">Step 3: Optionally you can verify that you are able to receive data from Event Hubs using the <a href="https://github.com/hdinsight/eventhubs-client" target="_blank">Java client for Event Hubs</a>.</span> &nbsp;</h4><p>There are two reasons you may want to do this:</p><ol>
<li>Become familiar with how Java client for Event Hubs works, what packages are needed, and how to build it etc, which can be helpful when we run Spark Streaming in Scala below</li>
<li>Make sure the ingestion pipeline works</li>
</ol><h1><span style="font-size: large">Processing data&nbsp;with Spark Streaming and Spark SQL on Cloudera:</span></h1><p>If you don't already have a Cloudera cluster, you can <a href="https://azure.microsoft.com/en-us/marketplace/partners/cloudera/clouderaedhcloudera/" target="_blank">install it from a simple UI wizard on Azure Marketplace</a>, which is also used in this example.&nbsp; But the cluster doesn't have to be in any specific environment, as long as you have Spark on YARN installed on the cluster. &nbsp;Please note that different versions of Spark do require some changes in the code. &nbsp;Here, we are using Spark 1.3.0 installed with Cloudera 5.4.</p><p>You can find the source code for this Spark job on <a href="https://github.com/liupeirong/Azure/tree/master/EventHubStream" target="_blank">GitHub</a>. &nbsp; Below are the key steps:</p><h2><span style="font-size: medium">Running the Spark job in Spark-Shell</span></h2><p>Before submitting a Spark job in cluster mode, it's a good idea to run Spark interactively with spark-shell for easy troubleshooting. &nbsp;In the YARN client mode, the Spark driver runs on the local machine, so any additional jars must be available on the local machine. &nbsp;Any additional jars required by the Spark executors must be available on the Spark executor nodes as well. &nbsp;For simplicity, we copied all the necessary jars to the /libs folder on both the Spark driver and the executors.</p><p style="padding-left: 30px"><span style="color: #993366"><em>spark-shell --master yarn --deploy-mode client --executor-cores 4 -usejavacp --jars /libs/spark-streaming-eventhubs_2.10-0.1.0.jar,/libs/eventhubs-client-0.9.1.jar,/libs/qpid-amqp-1-0-client-0.32.jar,/libs/qpid-amqp-1-0-client-jms-0.32.jar,/libs/qpid-amqp-1-0-common-0.32.jar,/libs/qpid-client-0.32.jar,/libs/qpid-common-0.32.jar --driver-class-path /libs/sqljdbc4.jar --conf spark.executor.extraClassPath=/libs/sqljdbc4.jar</em></span></p><ul>
<li>You can build spark-streaming-eventhubs jar from the&nbsp;<a href="https://github.com/hdinsight/spark-eventhubs" target="_blank">source code of the Event Hubs receiver for Spark Streaming</a>, or you can download it from <a href="https://github.com/hdinsight/hdinsight-spark-examples/tree/master/sparkstreaming/lib" target="_blank">this Even Hubs Spark Streaming example</a>.&nbsp;</li>
<li>You can build eventhubs-client jar from the source code of the <a href="https://github.com/hdinsight/eventhubs-client" target="_blank">Java client for Event Hubs</a>.</li>
<li>You can <a href="https://www.microsoft.com/en-us/download/details.aspx?id=19847" target="_blank">download</a> sqljdbc jar. &nbsp;</li>
<li>One important thing to note is that the number of executor cores must double the number of Event Hubs partitions so that the executor can receive data from Event Hubs and also process the data at the same time. &nbsp;If this is not set correctly, you will not see any error, but there will be no output data either. &nbsp;It will appear that the job is not doing anything.&nbsp;</li>
</ul><h4><span style="font-size: small"><strong>Step 1</strong>: Receive data in Spark Streaming from Event Hubs</span></h4><p>In the interactive command prompt of spark-shell, you can paste the following Scala code to run the Spark Streaming job, replacing the parameters with your own. &nbsp;&nbsp;</p><pre class="scroll"><em><code class="java">import org.apache.spark.SparkConf<br>import org.apache.spark.SparkContext<br>import org.apache.spark.streaming.eventhubs.EventHubsUtils<br>import org.apache.spark.streaming.{Seconds, StreamingContext}<br> <br>val streamBatchIntervalInSeconds = 60<br>val ehParams = Map[String, String](<br> "eventhubs.policyname" -&gt; "your event hubs rule name",<br> "eventhubs.policykey" -&gt; "your event hubs policy key",<br> "eventhubs.namespace" -&gt; "your event hubs namespace",<br> "eventhubs.name" -&gt; "your event hubs name",<br> "eventhubs.partition.count" -&gt; "2", //executor core count must be twice that of partition count<br> "eventhubs.consumergroup" -&gt; "your event hubs consumer group name",<br> "eventhubs.checkpoint.dir" -&gt; "your spark checkpoint dir in hdfs", //for simplicity we are not using reliable receiver with checkpoint in this example<br> "eventhubs.checkpoint.interval" -&gt; "600")<br>val ssc = new StreamingContext(sc, Seconds(streamBatchIntervalInSeconds))<br>val stream = EventHubsUtils.createUnionStream(ssc, ehParams)<br>val lines = stream.map(msg =&gt; new String(msg))<br>lines.print()<br>ssc.start()</code></em></pre><p>Start the telcodatagen mentioned earlier to generate some data for input. &nbsp;If you can see each line received from Event Hubs printed out in the spark-shell, congratulations, you have everything wired up correctly. &nbsp;</p><h4><span style="font-size: small"><strong>Step 2</strong>: Run real time queries in Spark SQL</span>&nbsp;&nbsp;</h4><p>Replace "<span style="color: #993366"><em>lines.print()</em></span>" in the above Scala code with the following code.</p><pre class="scroll"><em><code class="java"> //convert international datetime string to unix time<br> val isoformat = new java.text.SimpleDateFormat("yyyy-MM-dd'T'hh:mm:ss'Z'")<br> def date2long: (String =&gt; Long) = (s: String) =&gt; try { isoformat.parse(s).getTime() } catch { case _: Throwable =&gt; 0 }<br> val sqlfunc = udf(date2long)<br> <br> //define sqlContext<br> val sqlContext = new org.apache.spark.sql.SQLContext(sc)<br> import sqlContext.implicits._<br> import org.apache.spark.sql.functions._<br> <br> //process the data using Spark SQL<br> lines.foreachRDD { rdd =&gt; if (rdd.count() &gt; 0) {<br> val rawdf = sqlContext.jsonRDD(rdd);<br> // convert international time string to unix time for easy calculation<br> val df = rawdf.withColumn("callTime", sqlfunc(rawdf("callrecTime")));<br> // use the min time of this mini batch as the timestamp for the aggregated entry<br> val minTime = df.filter(df("callTime") &gt; 0).select("callTime").first().getLong(0);<br> // real time aggregation by region<br> val callsByRegion = df.groupBy("SwitchNum").count().withColumnRenamed("count", "callCount").withColumn("callTimeStamp", lit(minTime));<br> callsByRegion.show();<br> }<br> }</code></em></pre><p>Now you should be able to see aggregated results in the spark-shell console output. &nbsp;Note that the aggregation is applied directly on the Spark SQL DataFrame without having to run a SQL query. &nbsp;For more complex data analysis, such as detecting fraud in this example, we can run a query like the following:</p><pre class="scroll"><em><code class="java">df.registerTempTable("calls");<br>val fraudCalls = sqlContext.sql("SELECT CS1.CallingIMSI, CS1.callrecTime as calltime1, CS2.callrecTime as calltime2, CS1.CallingNum as CallingNum1, CS2.CallingNum as CallingNum2, CS1.SwitchNum as Switch1, CS2.SwitchNum as Switch2 FROM calls CS1 inner JOIN calls CS2 ON CS1.CallingIMSI = CS2.CallingIMSI AND (CS1.callTime - CS2.callTime) between 1000 AND 5000 WHERE CS1.SwitchNum != CS2.SwitchNum order by CS1.CallingIMSI, CS1.callrecTime");</code></em></pre><p>Note that this query will fail in Spark 1.3.0, probably related to a <a href="https://issues.apache.org/jira/browse/SPARK-6247" target="_blank">bug</a>&nbsp;with self joins in Spark SQL. &nbsp;We will demonstrate how to run this query in Impala instead later in this article.</p><h4><span style="font-size: small"><strong>Step 3</strong>: Store results in Azure SQL Server</span></h4><p>Create a table in Azure SQL Server:</p><pre style="padding-left: 30px"><span style="color: #993366"><em>CREATE TABLE callsByRegion (</em></span><br><span style="color: #993366"><em> SwitchNum NVARCHAR (20) NOT NULL,</em></span><br><span style="color: #993366"><em> callCount INT NOT NULL,</em></span><br><span style="color: #993366"><em> callTimeStamp BIGINT NOT NULL</em></span><br><span style="color: #993366"><em>);</em></span><br><span style="color: #993366"><em>CREATE CLUSTERED INDEX IX_callsByRegion</em></span><span style="color: #993366"><em> ON callsByRegion(callTimeStamp ASC);</em></span></pre><p>Add a jdbc connection string variable anywhere before "<span style="color: #993366"><em>lines.foreachRDD...</em></span>" in the above Scala code, replacing parameters with your own:</p><pre class="scroll"><em><code class="java">val jdbccxn = "jdbc:sqlserver://&lt;your azure db server&gt;.database.windows.net:1433;database=&lt;your db name&gt;;user=&lt;your db username&gt;;password=&lt;your password&gt;;encrypt=false;loginTimeout=30;"</code></em></pre><p>Replace "<span style="color: #993366"><em>callsByRegion.show()</em></span>" in the above Scala code with the following code:</p><pre class="scroll"><em><code class="java">//save real time aggregation to sql azure<br>callsByRegion.insertIntoJDBC(jdbccxn, "callsByRegion", false);</code></em></pre><p>Run the Spark job again and check your Azure SQL Server. &nbsp;You should see aggregated data in the callsByRegion table in your database.</p><h2><span style="font-size: medium">Running the Spark job with Spark-Submit</span></h2><p>Once everything runs fine in spark-shell, for production workload, you would want to submit the job to the Spark cluster. &nbsp;Using spark-submit, you can run the job either in client mode or cluster mode. &nbsp;You can build the Scala application into a jar and apply dependencies at run time, or you can build a uber jar that has all the dependencies in it. &nbsp;Check out this example on <a href="https://github.com/liupeirong/Azure/tree/master/EventHubStream" target="_blank">GitHub</a>&nbsp;to see how to build the jars in maven.</p><h4><span style="font-size: small">Running the job in YARN <strong>client mode</strong></span></h4><p>Note that the order of the parameters in the following commands is important. &nbsp;--class and app jar must be at the end of each command.&nbsp;</p><p>Using a jar without dependencies:</p><p style="padding-left: 30px"><em><span style="color: #993366">spark-submit --master yarn --deploy-mode client --executor-cores 4 --jars /libs/spark-streaming-eventhubs_2.10-0.1.0.jar,/libs/eventhubs-client-0.9.1.jar,/libs/qpid-amqp-1-0-client-0.32.jar,/libs/qpid-amqp-1-0-client-jms-0.32.jar,/libs/qpid-amqp-1-0-common-0.32.jar,/libs/qpid-client-0.32.jar,/libs/qpid-common-0.32.jar --driver-class-path /libs/sqljdbc4.jar --conf spark.executor.extraClassPath=/libs/sqljdbc4.jar --class com.sparkeventhub.sample.SparkEventHubSample ./app.jar</span></em></p><p>Using a uber jar with dependencies:</p><p style="padding-left: 30px"><span style="color: #993366"><em>spark-submit --master yarn --deploy-mode client --executor-cores 4 --driver-class-path /libs/sqljdbc4.jar --conf spark.executor.extraClassPath=/libs/sqljdbc4.jar --class com.sparkeventhub.sample.SparkEventHubSample ./app-jar-with-dependencies.jar</em></span></p><h4><span style="font-size: small">Running the job in YARN <strong>cluster mode</strong></span></h4><p>Using a jar without dependencies:</p><p style="padding-left: 30px"><span style="color: #993366"><em>spark-submit --master yarn --deploy-mode cluster&nbsp; --executor-cores 4 --driver-cores 2 --jars /libs/spark-streaming-eventhubs_2.10-0.1.0.jar,/libs/eventhubs-client-0.9.1.jar,/libs/qpid-amqp-1-0-client-0.32.jar,/libs/qpid-amqp-1-0-client-jms-0.32.jar,/libs/qpid-amqp-1-0-common-0.32.jar,/libs/qpid-client-0.32.jar,/libs/qpid-common-0.32.jar --driver-class-path /libs/sqljdbc4.jar --conf spark.executor.extraClassPath=/libs/sqljdbc4.jar --class com.sparkeventhub.sample.SparkEventHubSample ./app.jar</em></span></p><p>Using a uber jar with dependencies:</p><p style="padding-left: 30px"><em><span style="color: #993366">spark-submit --master yarn --deploy-mode cluster --executor-cores 4 --driver-cores 2 --driver-class-path /libs/sqljdbc4.jar --conf spark.executor.extraClassPath=/libs/sqljdbc4.jar --class com.sparkeventhub.sample.SparkEventHubSample ./app-jar-with-dependencies.jar</span></em></p><h1><span style="font-size: large">Processing data&nbsp;with Impala</span></h1><p>Instead of running queries in Spark SQL, we can alternatively store the data in hdfs, and run queries in real time with Impala. &nbsp;To store data in HDFS, add a variable to the output file path in HDFS anyhere before "<span style="color: #993366"><em>lines.foreachRDD...</em></span>" in the above Scala code, replacing the path with your own:</p><pre class="scroll"><em><code class="java">import org.apache.spark.sql.SaveMode<br>val outputDir = "sparkoutput/calls"</code></em></pre><p>Store the data in HDFS by adding the following code at the end of the "<span style="color: #993366"><em>lines.foreachRDD...</em></span>" block:</p><pre class="scroll"><em><code class="java">df.registerTempTable("rawcalls");<br>val calls=sqlContext.sql("SELECT callrecTime, SwitchNum, CallingIMSI, CallingNum, CalledNum, callTime from rawcalls");<br>calls.save(outputDir, SaveMode.Append);<br></code></em></pre><p>Run the job again, and you should be able to see parquet files being stored in the specified HDFS folder. &nbsp;The folder may be relative to the current HDFS user home directory, for example "/user/hdfs/sparkoutput/calls". &nbsp;Create an external table in Impala:</p><p style="padding-left: 30px"><em><span style="color: #993366">create external table calls (callrecTime string, SwitchNum string, CallingIMSI string, CallingNum string, CalledNum string, callTime bigint) stored as parquet location '/user/hdfs/sparkoutput/calls'</span></em></p><p>Query the data in Impala:</p><p style="padding-left: 30px"><span style="color: #993366"><em>SELECT CS1.CallingIMSI, CS1.callrecTime as calltime1, CS2.callrecTime as calltime2, CS1.CallingNum as CallingNum1, CS2.CallingNum as CallingNum2, CS1.SwitchNum as Switch1, CS2.SwitchNum as Switch2 FROM calls CS1 inner JOIN calls CS2 ON CS1.CallingIMSI = CS2.CallingIMSI AND (CS1.callTime - CS2.callTime) between 1000 AND 5000 WHERE CS1.SwitchNum != CS2.SwitchNum order by CS1.CallingIMSI, CS1.callrecTime</em></span></p><p>There you have it. &nbsp;I'm new to Spark Streaming, Spark SQL, and Impala. &nbsp;Would love to learn if there are better ways to do this.&nbsp;</p></p>
</div>
</div></body>
<script type='text/javascript' src='resource/jquery-1.12.1.min.js'></script>
<script type='text/javascript' src='resource/replace.js'></script>
</html>
